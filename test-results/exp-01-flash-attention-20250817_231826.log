🚀 開始最小化測試...
==================================================

📋 執行測試: 設備檢測

🔍 測試設備檢測...
  PyTorch 版本: 2.6.0
  MPS 可用: True
  MPS 已建置: True
  將使用設備: mps
  測試基本張量操作...
  張量形狀: torch.Size([10, 10])
✅ 基本張量操作通過

📋 執行測試: 依賴檢查

📦 測試關鍵依賴...
  ✅ nvidia-pyindex 未安裝（符合預期）
  ✅ nvidia-tensorrt 未安裝（符合預期）
  ✅ pycuda 未安裝（符合預期）
  ✅ torch: 2.6.0
  ✅ diffusers: 0.32.2
  ✅ transformers: 4.49.0
import error: No module named 'triton'
  ✅ accelerate: 1.4.0

📋 執行測試: Flash Attention狀態

⚡ 測試 Flash Attention 當前狀態...
❌ Flash Attention 不可用: No module named 'flash_attn'

📋 執行測試: 模組導入
🧪 測試模組導入...
⚠️  Flash Attention 不可用，使用智能回退
  ✅ ActionModule 導入成功
  ✅ ActionModule 初始化成功
  ✅ WanModel 導入成功
  ✅ causal_inference 導入成功
✅ 基本導入測試通過

==================================================
📊 測試結果總覽:
  設備檢測: ✅ 通過
  依賴檢查: ✅ 通過
  Flash Attention狀態: ❌ 失敗
  模組導入: ✅ 通過

🎯 總體結果: ❌ 部分測試失敗
