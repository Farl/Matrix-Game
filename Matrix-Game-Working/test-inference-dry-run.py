#!/usr/bin/env python3
"""
æ¨ç†åŠŸèƒ½ä¹¾ç‡¥é‹è¡Œæ¸¬è©¦ï¼šæ¸¬è©¦ä¸éœ€è¦å¯¦éš›æ¨¡å‹æª¢æŸ¥é»çš„æ¨ç†æµç¨‹
"""

import os
import sys
import torch
import traceback

# æ·»åŠ ç•¶å‰ç›®éŒ„åˆ° Python path
sys.path.insert(0, os.getcwd())

def test_inference_initialization():
    """æ¸¬è©¦æ¨ç†é¡åˆ¥æ˜¯å¦èƒ½æ­£ç¢ºåˆå§‹åŒ–ï¼ˆä¸è¼‰å…¥å¯¦éš›æ¨¡å‹ï¼‰"""
    print("ğŸ”¬ æ¸¬è©¦æ¨ç†åˆå§‹åŒ–...")
    
    try:
        # æ¨¡æ“¬åƒæ•¸
        class MockArgs:
            config_path = "configs/inference_yaml/inference_universal.yaml"
            checkpoint_path = ""  # ç©ºè·¯å¾‘ï¼Œæ¸¬è©¦éŒ¯èª¤è™•ç†
            img_path = "demo_images/universal/0000.png"
            output_folder = "test_outputs/"
            num_output_frames = 10  # å¾ˆå°çš„æ•¸é‡
            seed = 42
            pretrained_model_path = "Matrix-Game-2.0"  # ä¸å­˜åœ¨ï¼Œä½†æ¸¬è©¦åˆå§‹åŒ–
        
        from inference import InteractiveGameInference
        
        # é€™è£¡æ‡‰è©²æœƒåœ¨æ¨¡å‹è¼‰å…¥éšæ®µå¤±æ•—ï¼Œä½†æˆ‘å€‘å¯ä»¥æ¸¬è©¦è¨­å‚™æª¢æ¸¬
        try:
            inference = InteractiveGameInference(MockArgs())
            print(f"  âœ… è¨­å‚™æª¢æ¸¬æˆåŠŸ: {inference.device}")
            return {"status": "partial_success", "device": str(inference.device)}
        except FileNotFoundError as e:
            if "config" in str(e).lower():
                print(f"  âš ï¸  é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°ï¼ˆé æœŸï¼‰: {e}")
                return {"status": "config_missing", "error": str(e)}
            else:
                print(f"  âš ï¸  æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°ï¼ˆé æœŸï¼‰: {e}")
                return {"status": "model_missing", "error": str(e)}
        except Exception as e:
            print(f"  âš ï¸  åˆå§‹åŒ–éƒ¨åˆ†å¤±æ•—ï¼ˆå¯èƒ½æ­£å¸¸ï¼‰: {e}")
            return {"status": "init_failed", "error": str(e)}
            
    except ImportError as e:
        print(f"  âŒ å°å…¥å¤±æ•—: {e}")
        return {"status": "import_failed", "error": str(e)}

def test_streaming_inference_initialization():
    """æ¸¬è©¦ä¸²æµæ¨ç†é¡åˆ¥æ˜¯å¦èƒ½æ­£ç¢ºåˆå§‹åŒ–"""
    print("\nğŸŒŠ æ¸¬è©¦ä¸²æµæ¨ç†åˆå§‹åŒ–...")
    
    try:
        # æ¨¡æ“¬åƒæ•¸
        class MockArgs:
            config_path = "configs/inference_yaml/inference_universal.yaml"
            checkpoint_path = ""
            output_folder = "test_outputs/"
            max_num_output_frames = 10
            seed = 42
            pretrained_model_path = "Matrix-Game-2.0"
        
        from inference_streaming import InteractiveGameInference as StreamingInference
        
        try:
            inference = StreamingInference(MockArgs())
            print(f"  âœ… è¨­å‚™æª¢æ¸¬æˆåŠŸ: {inference.device}")
            return {"status": "partial_success", "device": str(inference.device)}
        except Exception as e:
            print(f"  âš ï¸  åˆå§‹åŒ–å¤±æ•—ï¼ˆå¯èƒ½æ­£å¸¸ï¼‰: {e}")
            return {"status": "init_failed", "error": str(e)}
            
    except ImportError as e:
        print(f"  âŒ å°å…¥å¤±æ•—: {e}")
        return {"status": "import_failed", "error": str(e)}

def test_device_compatibility():
    """æ¸¬è©¦è¨­å‚™ç›¸å®¹æ€§"""
    print("\nğŸ–¥ï¸  æ¸¬è©¦è¨­å‚™ç›¸å®¹æ€§...")
    
    device_info = {
        'mps_available': torch.backends.mps.is_available(),
        'cuda_available': torch.cuda.is_available(),
        'pytorch_version': torch.__version__
    }
    
    # æ¨¡æ“¬æ¨ç†è…³æœ¬ä¸­çš„è¨­å‚™æª¢æ¸¬é‚è¼¯
    if torch.backends.mps.is_available():
        selected_device = torch.device("mps")
        print("  âœ… å°‡ä½¿ç”¨ MPS è¨­å‚™")
    elif torch.cuda.is_available():
        selected_device = torch.device("cuda")
        print("  âœ… å°‡ä½¿ç”¨ CUDA è¨­å‚™")
    else:
        selected_device = torch.device("cpu")
        print("  âœ… å°‡ä½¿ç”¨ CPU è¨­å‚™")
    
    # æ¸¬è©¦ bfloat16 ç›¸å®¹æ€§
    try:
        if selected_device.type == "mps":
            # åœ¨ MPS ä¸Šæ¸¬è©¦ bfloat16
            test_tensor = torch.randn(10, 10, device=selected_device, dtype=torch.bfloat16)
            print("  âœ… MPS bfloat16 æ”¯æ´æ­£å¸¸")
        else:
            test_tensor = torch.randn(10, 10, device=selected_device, dtype=torch.bfloat16)
            print(f"  âœ… {selected_device.type} bfloat16 æ”¯æ´æ­£å¸¸")
    except Exception as e:
        print(f"  âš ï¸  bfloat16 ä¸æ”¯æ´: {e}")
    
    device_info['selected_device'] = str(selected_device)
    return device_info

def test_config_file_loading():
    """æ¸¬è©¦é…ç½®æ–‡ä»¶è¼‰å…¥"""
    print("\nğŸ“‹ æ¸¬è©¦é…ç½®æ–‡ä»¶è¼‰å…¥...")
    
    try:
        from omegaconf import OmegaConf
        
        config_files = [
            "configs/inference_yaml/inference_universal.yaml",
            "configs/inference_yaml/inference_gta_drive.yaml",
            "configs/inference_yaml/inference_templerun.yaml"
        ]
        
        results = {}
        for config_path in config_files:
            if os.path.exists(config_path):
                try:
                    config = OmegaConf.load(config_path)
                    print(f"  âœ… {config_path} è¼‰å…¥æˆåŠŸ")
                    results[config_path] = "success"
                except Exception as e:
                    print(f"  âŒ {config_path} è¼‰å…¥å¤±æ•—: {e}")
                    results[config_path] = f"failed: {e}"
            else:
                print(f"  âš ï¸  {config_path} ä¸å­˜åœ¨")
                results[config_path] = "missing"
        
        return results
        
    except ImportError as e:
        print(f"  âŒ OmegaConf å°å…¥å¤±æ•—: {e}")
        return {"error": f"import_failed: {e}"}

def main():
    print("ğŸš€ é–‹å§‹æ¨ç†åŠŸèƒ½ä¹¾ç‡¥é‹è¡Œæ¸¬è©¦...")
    print("=" * 60)
    
    test_results = {
        'device_compatibility': test_device_compatibility(),
        'config_loading': test_config_file_loading(),
        'inference_init': test_inference_initialization(),
        'streaming_init': test_streaming_inference_initialization()
    }
    
    print("\n" + "=" * 60)
    print("ğŸ“Š æ¸¬è©¦ç¸½çµ:")
    
    # è¨­å‚™ç›¸å®¹æ€§
    device_info = test_results['device_compatibility']
    print(f"  è¨­å‚™é¸æ“‡: {device_info.get('selected_device', 'unknown')}")
    print(f"  MPS å¯ç”¨: {device_info.get('mps_available', False)}")
    
    # é…ç½®æ–‡ä»¶
    config_results = test_results['config_loading']
    if isinstance(config_results, dict) and 'error' not in config_results:
        successful_configs = len([k for k, v in config_results.items() if v == "success"])
        print(f"  é…ç½®æ–‡ä»¶è¼‰å…¥: {successful_configs}/{len(config_results)} æˆåŠŸ")
    
    # æ¨ç†åˆå§‹åŒ–
    inference_result = test_results['inference_init']
    streaming_result = test_results['streaming_init']
    
    print(f"  æ¨™æº–æ¨ç†åˆå§‹åŒ–: {inference_result.get('status', 'unknown')}")
    print(f"  ä¸²æµæ¨ç†åˆå§‹åŒ–: {streaming_result.get('status', 'unknown')}")
    
    print("\nğŸ¯ çµè«–:")
    if device_info.get('mps_available', False):
        print("  âœ… Apple Silicon MPS è¨­å‚™æª¢æ¸¬æ­£å¸¸")
        print("  âœ… åŸºç¤æ¶æ§‹ç›¸å®¹")
        print("  âš ï¸  éœ€è¦ä¸‹è¼‰é è¨“ç·´æ¨¡å‹æ‰èƒ½é€²è¡Œå®Œæ•´æ¨ç†")
        print("  ğŸ“‹ è‡ªå‹•ä¸‹è¼‰: bash download_models.sh")
        print("  ğŸ“‹ æ‰‹å‹•ä¸‹è¼‰: huggingface-cli download Skywork/Matrix-Game-2.0 --local-dir Matrix-Game-2.0")
    else:
        print("  âšª é MPS ç’°å¢ƒï¼Œä½†æ¶æ§‹æ‡‰è©²ç›¸å®¹")
    
    return test_results

if __name__ == "__main__":
    results = main()