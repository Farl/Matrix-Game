#!/usr/bin/env python3
"""
å®Œæ•´æ•´åˆæ¸¬è©¦ï¼šæ¸¬è©¦æ‰€æœ‰ Apple Silicon é©é…æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import torch
import sys
import os
import time
import json
import traceback
from pathlib import Path

# æ·»åŠ ç•¶å‰ç›®éŒ„åˆ° Python path
sys.path.insert(0, os.getcwd())

def test_core_modules():
    """æ¸¬è©¦æ ¸å¿ƒæ¨¡çµ„å°å…¥å’Œåˆå§‹åŒ–"""
    print("ğŸ”§ æ¸¬è©¦æ ¸å¿ƒæ¨¡çµ„...")
    
    results = {}
    
    try:
        from wan.modules.action_module import ActionModule
        action_module = ActionModule(mouse_dim_in=2, keyboard_dim_in=6)
        results['ActionModule'] = {'status': 'success', 'message': 'å°å…¥å’Œåˆå§‹åŒ–æˆåŠŸ'}
        print("  âœ… ActionModule")
    except Exception as e:
        results['ActionModule'] = {'status': 'failed', 'error': str(e)}
        print(f"  âŒ ActionModule: {e}")
    
    try:
        from wan.modules.model import WanModel, sinusoidal_embedding_1d
        # æ¸¬è©¦ sinusoidal embeddingï¼ˆæˆ‘å€‘ä¿®å¾©çš„é‡é»ï¼‰
        device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
        pos = torch.arange(10, device=device)
        emb = sinusoidal_embedding_1d(64, pos)
        results['WanModel'] = {'status': 'success', 'message': f'æ¨¡å‹å’Œembeddingæ­£å¸¸ï¼Œè¼¸å‡ºå½¢ç‹€: {emb.shape}'}
        print(f"  âœ… WanModel + sinusoidal_embedding")
    except Exception as e:
        results['WanModel'] = {'status': 'failed', 'error': str(e)}
        print(f"  âŒ WanModel: {e}")
    
    try:
        from pipeline.causal_inference import get_current_action
        results['CausalInference'] = {'status': 'success', 'message': 'ç®¡é“æ¨¡çµ„å°å…¥æˆåŠŸ'}
        print("  âœ… CausalInference")
    except Exception as e:
        results['CausalInference'] = {'status': 'failed', 'error': str(e)}
        print(f"  âŒ CausalInference: {e}")
    
    return results

def test_device_compatibility():
    """æ¸¬è©¦è¨­å‚™ç›¸å®¹æ€§"""
    print("\nğŸ–¥ï¸  æ¸¬è©¦è¨­å‚™ç›¸å®¹æ€§...")
    
    device_info = {
        'mps_available': torch.backends.mps.is_available(),
        'mps_built': torch.backends.mps.is_built(),
        'cuda_available': torch.cuda.is_available(),
        'pytorch_version': torch.__version__
    }
    
    print(f"  PyTorch ç‰ˆæœ¬: {device_info['pytorch_version']}")
    print(f"  MPS å¯ç”¨: {device_info['mps_available']}")
    print(f"  MPS å»ºç½®: {device_info['mps_built']}")
    print(f"  CUDA å¯ç”¨: {device_info['cuda_available']}")
    
    # é¸æ“‡æœ€ä½³è¨­å‚™
    if device_info['mps_available']:
        device = torch.device("mps")
        print(f"  âœ… å°‡ä½¿ç”¨ MPS åŠ é€Ÿ")
    elif device_info['cuda_available']:
        device = torch.device("cuda")
        print(f"  âœ… å°‡ä½¿ç”¨ CUDA åŠ é€Ÿ")
    else:
        device = torch.device("cpu")
        print(f"  âšª å°‡ä½¿ç”¨ CPU")
    
    # æ¸¬è©¦åŸºæœ¬å¼µé‡æ“ä½œ
    try:
        x = torch.randn(100, 100, device=device)
        y = torch.matmul(x, x.T)
        z = torch.softmax(y, dim=-1)
        print(f"  âœ… åŸºæœ¬å¼µé‡æ“ä½œæ­£å¸¸")
        
        # æ¸¬è©¦è¨˜æ†¶é«”ä½¿ç”¨
        if device.type == 'mps':
            memory_used = torch.mps.current_allocated_memory() / 1024**2
            print(f"  ğŸ“Š MPS è¨˜æ†¶é«”ä½¿ç”¨: {memory_used:.1f}MB")
            
            # æ¸¬è©¦è¨˜æ†¶é«”æ¸…ç†
            del x, y, z
            torch.mps.empty_cache()
            memory_after = torch.mps.current_allocated_memory() / 1024**2
            print(f"  ğŸ§¹ æ¸…ç†å¾Œè¨˜æ†¶é«”: {memory_after:.1f}MB")
            
    except Exception as e:
        print(f"  âŒ å¼µé‡æ“ä½œå¤±æ•—: {e}")
        device_info['tensor_ops_error'] = str(e)
    
    return device_info, device

def test_attention_mechanisms():
    """æ¸¬è©¦æ³¨æ„åŠ›æ©Ÿåˆ¶"""
    print("\nâš¡ æ¸¬è©¦æ³¨æ„åŠ›æ©Ÿåˆ¶...")
    
    device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
    
    # æ¸¬è©¦æˆ‘å€‘çš„ Flash Attention å›é€€
    try:
        from wan.modules.action_module import flash_attn_func, FLASH_ATTN_AVAILABLE
        
        print(f"  Flash Attention å¯ç”¨: {FLASH_ATTN_AVAILABLE}")
        
        # æ¸¬è©¦æ³¨æ„åŠ›è¨ˆç®—
        batch_size, seq_len, num_heads, head_dim = 1, 64, 8, 32
        q = torch.randn(batch_size, seq_len, num_heads, head_dim, device=device)
        k = torch.randn(batch_size, seq_len, num_heads, head_dim, device=device)
        v = torch.randn(batch_size, seq_len, num_heads, head_dim, device=device)
        
        start_time = time.time()
        output = flash_attn_func(q, k, v, causal=True)
        end_time = time.time()
        
        print(f"  âœ… æ³¨æ„åŠ›è¨ˆç®—æˆåŠŸ")
        print(f"  ğŸ“Š è¼¸å‡ºå½¢ç‹€: {output.shape}")
        print(f"  â±ï¸  åŸ·è¡Œæ™‚é–“: {end_time - start_time:.4f}s")
        
        return {'status': 'success', 'execution_time': end_time - start_time}
        
    except Exception as e:
        print(f"  âŒ æ³¨æ„åŠ›æ©Ÿåˆ¶æ¸¬è©¦å¤±æ•—: {e}")
        return {'status': 'failed', 'error': str(e)}

def test_precision_and_stability():
    """æ¸¬è©¦ç²¾åº¦å’Œç©©å®šæ€§"""
    print("\nğŸ¯ æ¸¬è©¦ç²¾åº¦å’Œç©©å®šæ€§...")
    
    device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
    
    try:
        from wan.modules.model import sinusoidal_embedding_1d
        
        # æ¸¬è©¦ä¸åŒè¼¸å…¥å¤§å°çš„ç©©å®šæ€§
        test_sizes = [10, 100, 1000]
        results = {}
        
        for size in test_sizes:
            pos = torch.arange(size, device=device)
            emb = sinusoidal_embedding_1d(128, pos)
            
            # æª¢æŸ¥è¼¸å‡ºçš„æ•¸å€¼ç©©å®šæ€§
            has_nan = torch.isnan(emb).any().item()
            has_inf = torch.isinf(emb).any().item()
            value_range = (emb.min().item(), emb.max().item())
            
            results[f'size_{size}'] = {
                'shape': list(emb.shape),
                'has_nan': has_nan,
                'has_inf': has_inf,
                'value_range': value_range
            }
            
            status = "âœ…" if not (has_nan or has_inf) else "âŒ"
            print(f"  {status} å°ºå¯¸ {size}: ç¯„åœ {value_range[0]:.3f}~{value_range[1]:.3f}")
        
        return results
        
    except Exception as e:
        print(f"  âŒ ç²¾åº¦æ¸¬è©¦å¤±æ•—: {e}")
        return {'error': str(e)}

def test_memory_efficiency():
    """æ¸¬è©¦è¨˜æ†¶é«”æ•ˆç‡"""
    print("\nğŸ“Š æ¸¬è©¦è¨˜æ†¶é«”æ•ˆç‡...")
    
    if not torch.backends.mps.is_available():
        print("  âšª é MPS è¨­å‚™ï¼Œè·³éè¨˜æ†¶é«”æ¸¬è©¦")
        return {'skipped': True}
    
    device = torch.device("mps")
    
    try:
        # è¨˜éŒ„åˆå§‹è¨˜æ†¶é«”
        torch.mps.empty_cache()
        initial_memory = torch.mps.current_allocated_memory()
        print(f"  åˆå§‹è¨˜æ†¶é«”: {initial_memory / 1024**2:.1f}MB")
        
        # æ¨¡æ“¬è¼ƒå¤§çš„å¼µé‡æ“ä½œ
        tensors = []
        for i in range(10):
            tensor = torch.randn(1000, 1000, device=device)
            result = torch.matmul(tensor, tensor.T)
            tensors.append(result)
            
            current_memory = torch.mps.current_allocated_memory()
            print(f"  æ­¥é©Ÿ {i+1}: {current_memory / 1024**2:.1f}MB")
        
        peak_memory = torch.mps.current_allocated_memory()
        
        # æ¸…ç†è¨˜æ†¶é«”
        del tensors
        torch.mps.empty_cache()
        final_memory = torch.mps.current_allocated_memory()
        
        print(f"  å³°å€¼è¨˜æ†¶é«”: {peak_memory / 1024**2:.1f}MB")
        print(f"  æ¸…ç†å¾Œè¨˜æ†¶é«”: {final_memory / 1024**2:.1f}MB")
        
        # åˆ¤æ–·æ˜¯å¦åœ¨ M2 Mac é™åˆ¶ç¯„åœå…§ï¼ˆ18GBï¼‰
        within_limits = peak_memory < 18 * 1024**3
        status = "âœ…" if within_limits else "âš ï¸"
        print(f"  {status} M2 Mac ç›¸å®¹æ€§: {within_limits}")
        
        return {
            'initial_mb': initial_memory / 1024**2,
            'peak_mb': peak_memory / 1024**2,
            'final_mb': final_memory / 1024**2,
            'within_m2_limits': within_limits
        }
        
    except Exception as e:
        print(f"  âŒ è¨˜æ†¶é«”æ¸¬è©¦å¤±æ•—: {e}")
        return {'error': str(e)}

def generate_compatibility_report():
    """ç”Ÿæˆç›¸å®¹æ€§å ±å‘Š"""
    print("\nğŸ“ ç”Ÿæˆç›¸å®¹æ€§å ±å‘Š...")
    
    report = {
        'timestamp': time.time(),
        'test_environment': {
            'platform': 'Apple Silicon',
            'pytorch_version': torch.__version__,
            'mps_available': torch.backends.mps.is_available()
        },
        'test_results': {}
    }
    
    # åŸ·è¡Œæ‰€æœ‰æ¸¬è©¦
    print("\n" + "="*60)
    print("ğŸ§ª åŸ·è¡Œå®Œæ•´æ¸¬è©¦å¥—ä»¶")
    print("="*60)
    
    report['test_results']['core_modules'] = test_core_modules()
    report['test_results']['device_compatibility'], selected_device = test_device_compatibility()
    report['test_results']['attention_mechanisms'] = test_attention_mechanisms()
    report['test_results']['precision_stability'] = test_precision_and_stability()
    report['test_results']['memory_efficiency'] = test_memory_efficiency()
    
    # è¨ˆç®—ç¸½é«”æˆåŠŸç‡
    successful_tests = 0
    total_tests = 0
    
    for category, results in report['test_results'].items():
        if isinstance(results, dict):
            for test_name, test_result in results.items():
                if isinstance(test_result, dict) and 'status' in test_result:
                    total_tests += 1
                    if test_result['status'] == 'success':
                        successful_tests += 1
    
    success_rate = (successful_tests / total_tests * 100) if total_tests > 0 else 0
    report['overall_success_rate'] = success_rate
    
    # ä¿å­˜å ±å‘Š
    report_path = Path("../../test-results/integration-report.json")
    report_path.parent.mkdir(exist_ok=True)
    
    with open(report_path, 'w') as f:
        json.dump(report, f, indent=2, default=str)
    
    print(f"\nğŸ“Š æ¸¬è©¦å®Œæˆï¼")
    print(f"æˆåŠŸç‡: {success_rate:.1f}% ({successful_tests}/{total_tests})")
    print(f"å ±å‘Šå·²ä¿å­˜: {report_path.absolute()}")
    
    return report

def main():
    print("ğŸš€ Matrix Game 2.0 Apple Silicon æ•´åˆæ¸¬è©¦")
    print("="*60)
    
    try:
        report = generate_compatibility_report()
        
        print("\nğŸ¯ æœ€çµ‚è©•ä¼°:")
        if report['overall_success_rate'] >= 80:
            print("âœ… ç³»çµ±å·²æº–å‚™å¥½é€²å…¥ç”Ÿç”¢ç’°å¢ƒ")
            print("ğŸ‰ Apple Silicon é©é…æˆåŠŸï¼")
            
            # å‰µå»ºæˆåŠŸæ¨™è¨˜æ–‡ä»¶
            success_file = Path("../../test-results/all-experiments-passed")
            success_file.touch()
            
        else:
            print("âš ï¸  ç³»çµ±éœ€è¦é€²ä¸€æ­¥èª¿æ•´")
            print("ğŸ”§ è«‹æª¢æŸ¥å¤±æ•—çš„æ¸¬è©¦é …ç›®")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ æ¸¬è©¦åŸ·è¡Œå¤±æ•—: {e}")
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)